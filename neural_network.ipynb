{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adbd8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a75603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('codon_usage.csv',low_memory=False)\n",
    "df = df.drop(labels=[486,5063], axis=0)\n",
    "df['UUC'] = pd.to_numeric(df['UUC'], downcast=\"float\")\n",
    "#y = df.loc[:,'Kingdom']\n",
    "#X = df.iloc[:,6:69]\n",
    "\n",
    "#data = pd.concat([X, y])\n",
    "#print(pd.value_counts(y))\n",
    "#data = data.groupby(y).mean().plot.barh(stacked=True,legend=False);\n",
    "df = df[df[\"Kingdom\"].str.contains(\"plm\")==False]\n",
    "#y = df.loc[:,'Kingdom']\n",
    "#X = df.iloc[:,6:69]\n",
    "#data = pd.concat([X, y])\n",
    "#print(y.value_counts())\n",
    "#print(y.count())\n",
    "df['Kingdom'] = df[\"Kingdom\"].map({\"arc\": 0, \"bct\":1, \"phg\":2, \"pln\":3, \"inv\":4, \"vrt\":5, \n",
    "                                        \"mam\":6, \"rod\":7, \"pri\":8, \"vrl\":9})\n",
    "y = df.loc[:,'Kingdom'].to_numpy()\n",
    "X = df.iloc[:,6:69].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43532dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7804, 63)\n",
      "(2602, 63)\n",
      "(2602, 63)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ratio = 0.6\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_ratio/(train_ratio+test_ratio))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf630cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({1: 1778, 9: 1696, 3: 1508, 5: 1216, 4: 801, 6: 363, 7: 140, 2: 122, 8: 102, 0: 78})\n",
      "Resampled dataset shape Counter({0: 1778, 8: 1777, 2: 1776, 7: 1771, 6: 1743, 5: 1645, 4: 1613, 3: 1580, 1: 1528, 9: 1521})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN \n",
    "print('Original dataset shape %s' % Counter(y_train))\n",
    "sme = SMOTEENN(random_state=42)\n",
    "X_res, y_res = sme.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb5d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_res\n",
    "y_train=y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c0232",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a814e2e",
   "metadata": {},
   "source": [
    "### build first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39982590",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 63 \n",
    "hidden_layer_size = 32\n",
    "num_labels = 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbea4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer in a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in : int\n",
    "        Number of incomming connections.\n",
    "    \n",
    "    L_out : int\n",
    "        Number of outgoing connections. \n",
    "    \n",
    "    epsilon_init : float, optional\n",
    "        Range of values which the weight can take from a uniform \n",
    "        distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : array_like\n",
    "        The weight initialiatized to random values.  Note that W should\n",
    "        be set to a matrix of size(L_out, 1 + L_in) as\n",
    "        the first column of W handles the \"bias\" terms.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Initialize W randomly so that we break the symmetry while training\n",
    "    the neural network. Note that the first column of W corresponds \n",
    "    to the parameters for the bias unit.\n",
    "    \"\"\"\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "    # ============================================================\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4f3bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Network Parameters ...\n"
     ]
    }
   ],
   "source": [
    "print('Initializing Neural Network Parameters ...')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a69d6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z. \n",
    "    This should work regardless if z is a matrix or a vector. \n",
    "    In particular, if z is a vector or matrix, you should return\n",
    "    the gradient for each element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the gradient of the sigmoid function evaluated at\n",
    "    each value of z (z can be a matrix, vector or scalar).\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    We have provided an implementation of the sigmoid function \n",
    "    in `utils.py` file accompanying this assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    g = utils.sigmoid(z) * (1 - utils.sigmoid(z))\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78bd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function and gradient for a two layer neural \n",
    "    network which performs classification. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into \n",
    "        a vector. This needs to be converted back into the weight matrices Theta1\n",
    "        and Theta2.\n",
    "    \n",
    "    input_layer_size : int\n",
    "        Number of features for the input layer. \n",
    "    \n",
    "    hidden_layer_size : int\n",
    "        Number of hidden units in the second layer.\n",
    "    \n",
    "    num_labels : int\n",
    "        Total number of labels, or equivalently number of units in output layer. \n",
    "    \n",
    "    X : array_like\n",
    "        Input dataset. A matrix of shape (m x input_layer_size).\n",
    "    \n",
    "    y : array_like\n",
    "        Dataset labels. A vector of shape (m,).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        Regularization parameter.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function at the current weight values.\n",
    "    \n",
    "    grad : array_like\n",
    "        An \"unrolled\" vector of the partial derivatives of the concatenatation of\n",
    "        neural network weights Theta1 and Theta2.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    You should complete the code by working through the following parts.\n",
    "    \n",
    "    - Part 1: Feedforward the neural network and return the cost in the \n",
    "              variable J. After implementing Part 1, you can verify that your\n",
    "              cost function computation is correct by verifying the cost\n",
    "              computed in the following cell.\n",
    "    \n",
    "    - Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "              Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "              the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "              Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "              that your implementation is correct by running checkNNGradients provided\n",
    "              in the utils.py module.\n",
    "    \n",
    "              Note: The vector y passed into the function is a vector of labels\n",
    "                    containing values from 0..K-1. You need to map this vector into a \n",
    "                    binary vector of 1's and 0's to be used with the neural network\n",
    "                    cost function.\n",
    "     \n",
    "              Hint: We recommend implementing backpropagation using a for-loop\n",
    "                    over the training examples if you are implementing it for the \n",
    "                    first time.\n",
    "    \n",
    "    - Part 3: Implement regularization with the cost function and gradients.\n",
    "    \n",
    "              Hint: You can implement this around the code for\n",
    "                    backpropagation. That is, you can compute the gradients for\n",
    "                    the regularization separately and then add them to Theta1_grad\n",
    "                    and Theta2_grad from Part 2.\n",
    "    \n",
    "    Note \n",
    "    ----\n",
    "    We have provided an implementation for the sigmoid function in the file \n",
    "    `utils.py` accompanying this assignment.\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    \n",
    "    a2 = utils.sigmoid(a1.dot(Theta1.T))\n",
    "    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)\n",
    "    \n",
    "    a3 = utils.sigmoid(a2.dot(Theta2.T))\n",
    "    \n",
    "    y_matrix = y.reshape(-1)\n",
    "    y_matrix = np.eye(num_labels)[y_matrix]\n",
    "    \n",
    "    temp1 = Theta1\n",
    "    temp2 = Theta2\n",
    "    \n",
    "    # Add regularization term\n",
    "    \n",
    "    reg_term = (lambda_ / (2 * m)) * (np.sum(np.square(temp1[:, 1:])) + np.sum(np.square(temp2[:, 1:])))\n",
    "    \n",
    "    J = (-1 / m) * np.sum((np.log(a3) * y_matrix) + np.log(1 - a3) * (1 - y_matrix)) + reg_term\n",
    "    \n",
    "    # Backpropogation\n",
    "    \n",
    "    delta_3 = a3 - y_matrix\n",
    "    delta_2 = delta_3.dot(Theta2)[:, 1:] * sigmoidGradient(a1.dot(Theta1.T))\n",
    "\n",
    "    Delta1 = delta_2.T.dot(a1)\n",
    "    Delta2 = delta_3.T.dot(a2)\n",
    "    \n",
    "    # Add regularization to gradient\n",
    "\n",
    "    Theta1_grad = (1 / m) * Delta1\n",
    "    Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n",
    "    \n",
    "    Theta2_grad = (1 / m) * Delta2\n",
    "    Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n",
    "    \n",
    "   \n",
    "    \n",
    "    # ================================================================\n",
    "    # Unroll gradients\n",
    "    # grad = np.concatenate([Theta1_grad.ravel(order=order), Theta2_grad.ravel(order=order)])\n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c208d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00927825 -0.00927825]\n",
      " [-0.00559136 -0.00559136]\n",
      " [-0.02017486 -0.02017486]\n",
      " [-0.00585433 -0.00585433]\n",
      " [ 0.00889912  0.00889912]\n",
      " [ 0.01315402  0.01315402]\n",
      " [-0.01049831 -0.01049831]\n",
      " [-0.01910997 -0.01910997]\n",
      " [-0.00836011 -0.00836011]\n",
      " [ 0.01976123  0.01976123]\n",
      " [ 0.00811587  0.00811587]\n",
      " [-0.01515689 -0.01515689]\n",
      " [ 0.00762814  0.00762814]\n",
      " [ 0.00827936  0.00827936]\n",
      " [ 0.02014747  0.02014747]\n",
      " [ 0.00315079  0.00315079]\n",
      " [-0.00674798 -0.00674798]\n",
      " [-0.0109273  -0.0109273 ]\n",
      " [ 0.01262954  0.01262954]\n",
      " [ 0.01809234  0.01809234]\n",
      " [ 0.31454497  0.31454497]\n",
      " [ 0.14895477  0.14895477]\n",
      " [ 0.17770766  0.17770766]\n",
      " [ 0.14745891  0.14745891]\n",
      " [ 0.15953087  0.15953087]\n",
      " [ 0.14381027  0.14381027]\n",
      " [ 0.11105659  0.11105659]\n",
      " [ 0.03839516  0.03839516]\n",
      " [ 0.0775739   0.0775739 ]\n",
      " [ 0.03592373  0.03592373]\n",
      " [ 0.07350885  0.07350885]\n",
      " [ 0.03392626  0.03392626]\n",
      " [ 0.0974007   0.0974007 ]\n",
      " [ 0.04486928  0.04486928]\n",
      " [ 0.05899539  0.05899539]\n",
      " [ 0.03843063  0.03843063]\n",
      " [ 0.06015138  0.06015138]\n",
      " [ 0.03153997  0.03153997]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.34693e-11\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 1.000000): 6.772858 \n",
      "(for lambda = 3, this value should be about 0.576051)\n"
     ]
    }
   ],
   "source": [
    "#  Check gradients by running checkNNGradients\n",
    "lambda_ = 1\n",
    "utils.checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, _  = nnCostFunction(initial_nn_params, input_layer_size,\n",
    "                          hidden_layer_size, num_labels, X_train, y_train, lambda_)\n",
    "\n",
    "print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' % (lambda_, debug_J))\n",
    "print('(for lambda = 3, this value should be about 0.576051)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05489e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 500}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 0.3\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X_train, y_train, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b6af",
   "metadata": {},
   "source": [
    "### build second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67632e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26058534 -3.44892582  1.79080436 ...  1.88274598  1.24365585\n",
      "   4.38013257]\n",
      " [-0.27378612  3.37179038  1.70888211 ...  3.0388796   0.82195142\n",
      "  -2.92381017]\n",
      " [ 0.09866643 -0.39202636  4.41828259 ... -1.61131017  1.02866712\n",
      "   2.0232074 ]\n",
      " ...\n",
      " [ 0.12952906  1.40453885 -0.73447447 ...  1.37917151 -0.72570072\n",
      "   0.02056101]\n",
      " [-0.43346546  2.79007308  0.04965792 ...  0.16040485  0.05782365\n",
      "  -0.53121348]\n",
      " [-0.29399809 -3.77389085  0.8864798  ... -0.43325231  1.29615034\n",
      "   5.59418873]]\n",
      "(32, 64)\n",
      "[[0.02256    0.0384     0.01715    ... 0.00186    0.0013     0.00261   ]\n",
      " [0.01262    0.03824    0.01109    ... 0.00191    0.         0.        ]\n",
      " [0.01797    0.04105    0.01594    ... 0.00176    0.00074    0.00104   ]\n",
      " ...\n",
      " [0.02101646 0.01418457 0.01711674 ... 0.00065799 0.00135597 0.000329  ]\n",
      " [0.0126689  0.03187748 0.02008067 ... 0.0003823  0.00090413 0.00036496]\n",
      " [0.02036344 0.01830769 0.0182807  ... 0.00252    0.00114361 0.00045901]]\n",
      "(16732, 63)\n",
      "[[1.00000000e+00 2.25600004e-02 3.84000000e-02 ... 1.86000000e-03\n",
      "  1.30000000e-03 2.61000000e-03]\n",
      " [1.00000000e+00 1.26200002e-02 3.82400000e-02 ... 1.91000000e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 1.79699995e-02 4.10500000e-02 ... 1.76000000e-03\n",
      "  7.40000000e-04 1.04000000e-03]\n",
      " ...\n",
      " [1.00000000e+00 2.10164648e-02 1.41845661e-02 ... 6.57991064e-04\n",
      "  1.35597268e-03 3.28995532e-04]\n",
      " [1.00000000e+00 1.26689043e-02 3.18774845e-02 ... 3.82298792e-04\n",
      "  9.04131259e-04 3.64962866e-04]\n",
      " [1.00000000e+00 2.03634421e-02 1.83076870e-02 ... 2.52000268e-03\n",
      "  1.14360946e-03 4.59010234e-04]]\n",
      "(16732, 64)\n",
      "[[0.51455232 0.5375511  0.47826746 ... 0.4645413  0.26201472 0.48413302]\n",
      " [0.50895521 0.54813056 0.52736323 ... 0.47616225 0.28126512 0.50163713]\n",
      " [0.49348379 0.52989326 0.50583922 ... 0.47257329 0.25423954 0.47408239]\n",
      " ...\n",
      " [0.51246641 0.41846045 0.47988313 ... 0.49901426 0.34280408 0.47614938]\n",
      " [0.53105158 0.50320467 0.49499277 ... 0.48132964 0.31373801 0.49134085]\n",
      " [0.52155229 0.46038669 0.49022205 ... 0.47695937 0.35593309 0.47604867]]\n",
      "(16732, 32)\n"
     ]
    }
   ],
   "source": [
    "print(Theta1)\n",
    "print(Theta1.shape)\n",
    "print(X_train)\n",
    "print(X_train.shape)\n",
    "a1 = np.concatenate([np.ones((16732, 1)), X_train], axis=1)\n",
    "print(a1)\n",
    "print(a1.shape)\n",
    "a2 = utils.sigmoid(a1.dot(Theta1.T))\n",
    "print(a2)\n",
    "print(a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2901a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size_2  = 32 \n",
    "hidden_layer_size_2 = 32\n",
    "num_labels_2 = 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c47fd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Network Parameters ...\n"
     ]
    }
   ],
   "source": [
    "print('Initializing Neural Network Parameters ...')\n",
    "\n",
    "initial_Theta1_2 = randInitializeWeights(input_layer_size_2, hidden_layer_size_2)\n",
    "initial_Theta2_2 = randInitializeWeights(hidden_layer_size_2, num_labels_2)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params_2 = np.concatenate([initial_Theta1_2.ravel(), initial_Theta2_2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8def4b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 600}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 0.3\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size_2,\n",
    "                                        hidden_layer_size_2,\n",
    "                                        num_labels_2, a2, y_train, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params_2,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params_2 = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1_2 = np.reshape(nn_params_2[:hidden_layer_size_2 * (input_layer_size_2 + 1)],\n",
    "                    (hidden_layer_size_2, (input_layer_size_2 + 1)))\n",
    "\n",
    "Theta2_2 = np.reshape(nn_params_2[(hidden_layer_size_2 * (input_layer_size_2 + 1)):],\n",
    "                    (num_labels_2, (hidden_layer_size_2 + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2a180",
   "metadata": {},
   "source": [
    "### build third layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d734edb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16732, 32)\n",
      "(32, 33)\n"
     ]
    }
   ],
   "source": [
    "print(a2.shape)\n",
    "print(Theta1_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c9b40923",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_3 = np.concatenate([np.ones((16732, 1)), a2], axis=1)\n",
    "a2_3 = utils.sigmoid(a1_3.dot(Theta1_2.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "066848da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size_2  = 32 \n",
    "hidden_layer_size_2 = 32\n",
    "num_labels_2 = 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fbfe57a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Network Parameters ...\n"
     ]
    }
   ],
   "source": [
    "print('Initializing Neural Network Parameters ...')\n",
    "\n",
    "initial_Theta1_2 = randInitializeWeights(input_layer_size_2, hidden_layer_size_2)\n",
    "initial_Theta2_2 = randInitializeWeights(hidden_layer_size_2, num_labels_2)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params_2 = np.concatenate([initial_Theta1_2.ravel(), initial_Theta2_2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7418929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 500}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 0.8\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size_2,\n",
    "                                        hidden_layer_size_2,\n",
    "                                        num_labels_2, a2_3, y_train, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params_2,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params_3 = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1_3 = np.reshape(nn_params_3[:hidden_layer_size_2 * (input_layer_size_2 + 1)],\n",
    "                    (hidden_layer_size_2, (input_layer_size_2 + 1)))\n",
    "\n",
    "Theta2_3 = np.reshape(nn_params_3[(hidden_layer_size_2 * (input_layer_size_2 + 1)):],\n",
    "                    (num_labels_2, (hidden_layer_size_2 + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212435e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdc0bd7a",
   "metadata": {},
   "source": [
    "## evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2576a",
   "metadata": {},
   "source": [
    "### evaluate the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f5e753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 88.775998\n",
      "Validation Set Accuracy: 81.322060\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.96      1.00      0.98      1778\n",
      "     class 1       0.92      0.89      0.90      1528\n",
      "     class 2       0.94      0.97      0.95      1776\n",
      "     class 3       0.89      0.87      0.88      1580\n",
      "     class 4       0.84      0.81      0.83      1613\n",
      "     class 5       0.96      0.90      0.93      1645\n",
      "     class 6       0.87      0.79      0.83      1743\n",
      "     class 7       0.79      0.88      0.83      1771\n",
      "     class 8       0.87      0.87      0.87      1777\n",
      "     class 9       0.85      0.89      0.87      1521\n",
      "\n",
      "    accuracy                           0.89     16732\n",
      "   macro avg       0.89      0.89      0.89     16732\n",
      "weighted avg       0.89      0.89      0.89     16732\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.45      0.96      0.61        26\n",
      "     class 1       0.95      0.82      0.88       552\n",
      "     class 2       0.40      0.83      0.54        46\n",
      "     class 3       0.84      0.81      0.82       491\n",
      "     class 4       0.64      0.71      0.67       285\n",
      "     class 5       0.96      0.82      0.88       428\n",
      "     class 6       0.81      0.72      0.76       115\n",
      "     class 7       0.32      0.69      0.44        35\n",
      "     class 8       0.51      0.89      0.65        37\n",
      "     class 9       0.89      0.87      0.88       587\n",
      "\n",
      "    accuracy                           0.81      2602\n",
      "   macro avg       0.67      0.81      0.71      2602\n",
      "weighted avg       0.85      0.81      0.82      2602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_train = utils.predict(Theta1, Theta2, X_train)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred_train == y_train) * 100))\n",
    "pred_valid = utils.predict(Theta1, Theta2, X_valid)\n",
    "print('Validation Set Accuracy: %f' % (np.mean(pred_valid == y_valid) * 100))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, pred_train, target_names=['class 0', 'class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9']))\n",
    "print(classification_report(y_valid, pred_valid, target_names=['class 0', 'class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6feaa",
   "metadata": {},
   "source": [
    "### evaluate the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9eb73ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 90.718384\n",
      "Validation Set Accuracy: 83.243659\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.97      1.00      0.98      1778\n",
      "     class 1       0.93      0.91      0.92      1528\n",
      "     class 2       0.94      0.97      0.96      1776\n",
      "     class 3       0.90      0.89      0.89      1580\n",
      "     class 4       0.88      0.86      0.87      1613\n",
      "     class 5       0.97      0.94      0.95      1645\n",
      "     class 6       0.87      0.82      0.84      1743\n",
      "     class 7       0.82      0.88      0.85      1771\n",
      "     class 8       0.90      0.89      0.90      1777\n",
      "     class 9       0.90      0.90      0.90      1521\n",
      "\n",
      "    accuracy                           0.91     16732\n",
      "   macro avg       0.91      0.91      0.91     16732\n",
      "weighted avg       0.91      0.91      0.91     16732\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.50      0.96      0.66        26\n",
      "     class 1       0.95      0.83      0.88       552\n",
      "     class 2       0.43      0.83      0.56        46\n",
      "     class 3       0.84      0.82      0.83       491\n",
      "     class 4       0.68      0.73      0.70       285\n",
      "     class 5       0.95      0.89      0.92       428\n",
      "     class 6       0.77      0.72      0.74       115\n",
      "     class 7       0.47      0.77      0.59        35\n",
      "     class 8       0.52      0.81      0.63        37\n",
      "     class 9       0.91      0.87      0.89       587\n",
      "\n",
      "    accuracy                           0.83      2602\n",
      "   macro avg       0.70      0.82      0.74      2602\n",
      "weighted avg       0.85      0.83      0.84      2602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_train = utils.predict(Theta1_2, Theta2_2, a2)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred_train == y_train) * 100))\n",
    "\n",
    "a1_v = np.concatenate([np.ones((2602, 1)), X_valid], axis=1)\n",
    "a2_v = utils.sigmoid(a1_v.dot(Theta1.T))\n",
    "pred_valid = utils.predict(Theta1_2, Theta2_2, a2_v)\n",
    "print('Validation Set Accuracy: %f' % (np.mean(pred_valid == y_valid) * 100))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, pred_train, target_names=['class 0', 'class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9']))\n",
    "print(classification_report(y_valid, pred_valid, target_names=['class 0', 'class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826b611",
   "metadata": {},
   "source": [
    "### evaluate the third layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "52f08f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 92.523309\n",
      "Validation Set Accuracy: 84.358186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.98      1.00      0.99      1778\n",
      "     class 1       0.94      0.93      0.93      1528\n",
      "     class 2       0.95      0.98      0.96      1776\n",
      "     class 3       0.92      0.90      0.91      1580\n",
      "     class 4       0.89      0.89      0.89      1613\n",
      "     class 5       0.96      0.95      0.96      1645\n",
      "     class 6       0.90      0.86      0.88      1743\n",
      "     class 7       0.86      0.91      0.89      1771\n",
      "     class 8       0.94      0.92      0.93      1777\n",
      "     class 9       0.92      0.92      0.92      1521\n",
      "\n",
      "    accuracy                           0.93     16732\n",
      "   macro avg       0.93      0.92      0.92     16732\n",
      "weighted avg       0.93      0.93      0.93     16732\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.56      0.92      0.70        26\n",
      "     class 1       0.95      0.85      0.89       552\n",
      "     class 2       0.45      0.83      0.58        46\n",
      "     class 3       0.85      0.83      0.84       491\n",
      "     class 4       0.67      0.76      0.71       285\n",
      "     class 5       0.94      0.90      0.92       428\n",
      "     class 6       0.76      0.73      0.75       115\n",
      "     class 7       0.52      0.71      0.60        35\n",
      "     class 8       0.55      0.76      0.64        37\n",
      "     class 9       0.93      0.88      0.90       587\n",
      "\n",
      "    accuracy                           0.84      2602\n",
      "   macro avg       0.72      0.82      0.75      2602\n",
      "weighted avg       0.86      0.84      0.85      2602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_train = utils.predict(Theta1_3, Theta2_3, a2_3)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred_train == y_train) * 100))\n",
    "\n",
    "a1_3v = np.concatenate([np.ones((2602, 1)), a2_v], axis=1)\n",
    "a2_3v = utils.sigmoid(a1_3v.dot(Theta1_2.T))\n",
    "pred_valid = utils.predict(Theta1_3, Theta2_3, a2_3v)\n",
    "print('Validation Set Accuracy: %f' % (np.mean(pred_valid == y_valid) * 100))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, pred_train, target_names=['class 0', 'class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9']))\n",
    "print(classification_report(y_valid, pred_valid, target_names=['class 0', 'class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057e886",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6d39a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 84.396618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.38      0.73      0.50        22\n",
      "     class 1       0.94      0.86      0.90       589\n",
      "     class 2       0.49      0.81      0.61        52\n",
      "     class 3       0.87      0.83      0.85       524\n",
      "     class 4       0.67      0.75      0.71       259\n",
      "     class 5       0.96      0.90      0.93       433\n",
      "     class 6       0.79      0.70      0.74        94\n",
      "     class 7       0.48      0.65      0.55        40\n",
      "     class 8       0.58      0.80      0.67        41\n",
      "     class 9       0.90      0.89      0.90       548\n",
      "\n",
      "    accuracy                           0.84      2602\n",
      "   macro avg       0.71      0.79      0.74      2602\n",
      "weighted avg       0.86      0.84      0.85      2602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a1_t = np.concatenate([np.ones((2602, 1)), X_test], axis=1)\n",
    "a2_t = utils.sigmoid(a1_t.dot(Theta1.T))\n",
    "a1_3t = np.concatenate([np.ones((2602, 1)), a2_t], axis=1)\n",
    "a2_3t = utils.sigmoid(a1_3t.dot(Theta1_2.T))\n",
    "pred_test = utils.predict(Theta1_3, Theta2_3, a2_3t)\n",
    "print('Test Set Accuracy: %f' % (np.mean(pred_test == y_test) * 100))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_test, target_names=['class 0', 'class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9'],output_dict=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d84d9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report=classification_report(y_test, pred_test, target_names=['class 0', 'class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9'],output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0a4bd99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('data.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b006d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036320b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f739d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2c270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdaae24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3c0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93016c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "347010a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(nnCostFunction, X, y, lambda_=0, maxiter=100):\n",
    "    \n",
    "    # Create \"short hand\" for the cost function to be minimized\n",
    "    costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "    # Now, costFunction is a function that takes in only one argument\n",
    "    options = {'maxiter': maxiter}\n",
    "\n",
    "    # Minimize using scipy\n",
    "    res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "    return res.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9e42b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(X, y, Xval, yval, lambda_=0):\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = y.size\n",
    "    \n",
    "    # You need to return these values correctly\n",
    "    error_train = np.zeros(m)\n",
    "    error_val   = np.zeros(m)\n",
    "\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        \n",
    "        nn_params = trainNN(nnCostFunction, X[:i], y[:i], lambda_=lambda_)\n",
    "        \n",
    "        error_train[i - 1], _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X[:i], y[:i], lambda_=0)\n",
    "        \n",
    "        error_val[i - 1], _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, Xval, yval, lambda_=0)\n",
    "        \n",
    "    return error_train, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "error_train, error_val = learningCurve(X_train, y_train, X_valid, y_valid, lambda_=0.3)\n",
    "\n",
    "pyplot.plot(np.arange(1, y.size+1), error_train, np.arange(1, y.size+1), error_val, lw=2)\n",
    "pyplot.title('Learning curve for neural network')\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "pyplot.xlabel('Number of training examples')\n",
    "pyplot.ylabel('Error')\n",
    "pyplot.axis([0, y.size, 0, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54299da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c364547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67addd74",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Dec  7 2022, 10:15:13) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
